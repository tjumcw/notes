# 项目总结

### ==1、项目介绍与整理==

#### 1、项目介绍

对于这个项目我还是比较有自信和成就感的，我花了两个月的时间啥也没干沉浸式学习和写代码这个项目也是网上公认的难度相当大的项目。因为这个是麻省理工计算机专业分布式系统课程的结课大项目，需要完成总共4个项目，以跑测试用例通过率来评判得分，我是用C++写的，应该也是目前第一个开源的C++实现版本，可以保证是我自己实现的，不过我看已经被别人fork过了，您有兴趣的话也可以看看，因为都是封装的实现类，可以拿过来直接做基础模块。

我的项目可能和别人的项目差别比较大，没有基于任何现有的框架，所以在应用层做的工作比较少，基本是在底层的高可用、高性能和服务端做的工作，有点像写框架反正是造轮子的工作，总体代码量在5500行左右，都是自己一行一行敲的，我不知道别人项目工作量大不大，他们自己写的代码有多少，像我身边的人可能也就是花个一周或者10天照着视频或者现有代码就能完成一个项目，当然前期需要学习框架也可能要花费一些时间，反正我这个项目没用到框架，只有一个官方的需求文档，唯一的参考资料就是论文，花了2个月时间静下心来自己从零写的代码，也是感谢美团的面试官在实习面试中让我意识到自己的不足，4月20多号结束的面试吧，我从5月到7月就一直在弄。

额，谈的多了，项目主要工作集中在第二个项目Raft算法以及最后一个基于分片的分布式数据库。

##### 1）MapReduce

第一个项目MapReduce现在看来是属于练手性质的项目，大概在700行，相当于要复现谷歌2003年一篇论文中针对大数据提出的编程框架MapReduce，我做了论文中分布式处理超大文本词频统计的工作，是拿来熟练RPC通信以及分布式环境下服务器和客户端的处理方式，当然我是去实现了这个编程框架，可以支持其他任务比如倒排索引等等，因为其中最关键的MR函数我是通过运行时加载的动态库来实现词频统计的，只要针对不同的任务写不同的MR函数运行时加载就可以了，变化延迟到运行时，大概花了一个星期完成。



##### 2）Raft

第二个项目Raft是整个项目的核心，需要实现Raft类，作为后续分布式数据库的底层模块，代码量大概在1200行。Raft是一种分布式一致性协议，是一篇博士论文，主要就是反复阅读论文理解算法实现再通过代码编写这样的模块，具体实现依照论文主要分为3块，领导选举、日志同步、持久化（结合应用层快照）。简单说来就是我写的Raft类实现了类似Redis中集群配置中的主从复制和哨兵模式（leader发起心跳，不是哨兵）的功能，同时持久化和快照以及日志压缩的功能可以理解成Redis中的RDB以及AOF，同样是记录快照和日志，日志过大时根据当前状态机压缩日志。Raft主要保证了分布式环境下集群间服务器不超过半数节点宕机依旧可以正常工作，也就是常说的高可用性。这个模块封装好了，我也做了大量测试可以直接拿过来用，在我的github上有，对外只留下了两个接口，一个是应用层传入命令（比如说后续项目中的客户端发起的数据库请求等，这些会写入日志）的接口，另一个是我通过有名管道结合信号量实现的一个消息队列，用来按序存取和执行日志记录的数据库命令。上层应用层只需传入客户端对于数据库的操作指令，待底层的Raft层达成共识后通过这个消息队列传达给上层，在应用层的后台守护线程中执行命令更新状态机即可。代码量大概在1200行。



##### 3）KvRaft

前两个项目复现论文即可，后面两个项目就需要自己设计。第三个项目是基于raft实现的的单集群分布式数据库，所有客户端请求都由一个集群来受理，集群间的每个服务器通过raft保持一致性和高可用，整个集群对客户端需要保证并发请求的强一致性以及读请求的幂等性。这个数据库不是MySQL或者Redis，就是我自己在数据库类中维护的一个map，相当于是一个运行时的数据库，当然我在应用层做了快照，包括Raft层的持久化，两种方式保证重新启动时能快速恢复状态机。实现的数据库的Server类和一个Client类，Client处理相对简单，主要封装了3种请求方法GET、SET、APPEND，通过RPC方式轮询集群的leader节点，一旦成功缓存对应节点下次直接向他发请求即可（领导选举是随机的）。Server类中组合了一个Raft对象，集群间每个server节点都有一个raft对象，raft在底层完成领导选举、日志同步和持久化，raft的leader对应的server即是集群的leader。每个server会维护一个map，存key和value，还会维护一个客户端ID到客户端当前请求序号的map，以及一个客户端ID对应到请求上下文信息的map，这两个map保证请求的一致性和幂等性。关键点是客户端ID必须唯一且每个请求必须自增，发起RPC请求在服务端Leader节点受理时生成关于该客户端请求内容及客户端信息的上下文，分别存到上述两个map中，再将请求通过Raft接口传入Raft层达成一致性状态后通过消息队列到达应用层执行该命令，将数据写入数据库。Raft层日志过大会触发应用层快照，生成当前数据库及几个map的信息，写入磁盘并压缩Raft层日志。代码量大概在800行左右。



##### 4）shardKv

第四个项目是最难的综合项目，是在第三个项目的基础上，实现一个多集群的分布式数据库，相当于在之前保证强一致性和高可用性的分布式数据库基础上引入高性能的处理，代码量大概在2800行左右。其中，集群间是分布式环境，集群内各个server也是分布式的，在Raft保证集群内的高可用和一致性的基础上，还需要在集群间保证数据和客户端请求的一致性。同时，针对集群和分片的配置变化，需要完成负载均衡以及不同集群间数据的迁移和垃圾清理。

###### 1、配置管理相关

其中涉及到不同的集群受理不同的分片请求，有点类似一致性哈希但不全相同，主要就是客户端请求的key通过哈希函数后对分片数取模，将不同请求散列到不同的集群上。由于集群负载动态变化，可能会加入新的集群，或者工作集群不工作了退出集群，涉及到不同的集群和受理分片的信息变化，需要有一个配置类记录这个信息，也需要有一个配置控制器来实现上述提及的配置变更情况，同时完成分片的负载均衡。所有第四个最终项目分为两块，第一块就是按照项目3的思想设计一个配置控制服务器以及配置控制客户端，客户端的请求包括Join、Leave、Move以及Query，对于请求发起RPC至服务端，服务端处理，完全类似于项目3的整体框架，无非是请求的种类发生了变化，处理的逻辑发生了变化。一旦发生了配置更新，就需要进行分片的负载均衡，我是通过一个大根堆和一个小根堆来实现的，基本思想就是把当前负载最大的集群对应的分片分出来一个给当前负载最小的集群，直到最大负载和最小负载不超过1或者相等（看几个集群情况不一样）。这一块代码量大概在1100行。

###### 2、数据库相关

实现多集群的分布式数据库，每个Server类里除了自己本身的方法和成员还有一个上述配置器客户端对象以及一个Raft对象。客户端对象用于一个守护线程不断Query查询当前配置，配置类中有一个配置号的字段，如果当前配置不一样会将配置信息传入Raft层达成一致后更新配置信息并进行后台的负载均衡，这种情况下各个集群和负责分片的对应关系由于负载均衡发生了变化，集群间需要进行数据的迁移，主要包括需要获取新的分片数据，以及需要清理不再受理的分片数据。我的视线时通过pull模式去拉取自己需要负责的分片数据，收到数据后回复一个RPC告知被拉取端，被拉取端清理对应数据。（这一段处理很难，逻辑清晰实现也很复杂，是参考了网上的博客去做的，包括数据迁移过程中被拉取的分片对应的数据要保证不可用，保证数据完成迁移后才能删除不再受理的分片数据，同时需要新增两个守护线程检测是否需要拉取分片数据以及是否需要清理分片数据，一旦需要数据变动都是通过写入Raft层再提交应用层处理，应用层的执行命令的守护线程需要新增很多分支，关键就是要维持多集群整体的数据库对于客户端请求的一致性）。通过多集群的方式提高分布式环境下的性能，同时还需要保证集群内各节点的一致性以及集群间整体数据库的一致性。这一块代码在1700行左右。



##### 5）整体

坦白说我每次跟面试官聊项目都很难聊清楚，我也不知道为什么，我想了下可能还是我做的侧重点和别人不一样，我在应用层做的工作不多，反而有点象是在造轮子，别人可能基于什么框架做了什么改进，我这个是纯自己手写这个基本的类似框架的东西，比如最简单的MR，700行一个星期做完了，但实际上让我去讲我觉得不是短短20分钟能讲清楚的，因为我是一个实现者去实现这个编程框架，别人可能基于MR做了什么事，我是去实现了这个MR，要我去讲怎么实现我觉得很难在20分钟可以讲清楚，更何况后面的Raft算法细节很多，Raft类的实现在1200行，相当于复现博士论文提出的理论，我学习学了那么久，短时间不可能讲的清楚的。每次都变成实现了一个保证高可用的分布式一致性算法，但实际上这样一句话完全体现不出我在这方面花的心血。后面两个项目更是没有论文参考值有需求文档，都需要基于raft实现。别人项目基于什么mvc，boot，mybatis，cloud实现的功能很丰富，我这个其实都是底层实现，和服务端实现，感觉每次和面试官聊都很难体现出我的工作量。



#### 2、项目细节

主要分为协调器类以及工作线程，工作线程分map和reduce，因为是分布式环境，所有的工作线程模拟不同的服务器都是互相不知道的，所以需要一个协调器管理总体任务。协调器负责分发和管理所有任务状态，绑定了很多RPC服务函数，初始化时将所有任务读到工作队列中，等待工作线程通过RPC调用获得文件名，分配出去就将该文件加入map正在处理队列，用于超时管理及重发，简单处理就是创建线程加join方式等待即可。



#### ==3、精炼介绍==

这个项目是我比较自信的项目，因为我花了整整两个月的时间，啥也没干做这个项目，代码量大概在5500行，都是我一行一行写的，已经在GitHub开源。整个项目就用了一个轻量级的RPC框架，没有用其余任何框架，相当于是自己写框架然后基于写的框架去写应用层。

比如前两个项目就是在写框架，第一个项目我实现了谷歌提出的针对大数据的分布式编程框架MapReduce，其中关键的Map函数和Reduce函数是运行时从动态库加载进来的，我实现的是对超大文本词频统计的函数，可以针对其他任何不同的任务编写不同的函数，运行时加载即可，其他的协调类和工作线程的处理逻辑基本都是一样的，任务分派、任务定时、状态检测、超时重发等等都写好了，都是服务端实现RPCHandler，工作线程发起RPC请求完成通信。想实现其他功能直接写对应的动态库即可，代码量大概在700行，花了一个星期做完。

第二个项目Raft是分布式一致性算法，保证的是集群间的高可用，我手写了这个算法，也经过我自己大量测试，代码量在1200行左右。可以简单理解成我实现了类似Redis集群中的主从复制、哨兵模式、RDB和AOF的持久化这几个功能。当有也有些许不同，比如Raft是主节点管理心跳而不是另外一个哨兵进程。这相当于是写底层协议和框架的工作，很重要是后面两个项目的底层模块，而且里面实现起来细节很多，主要分为领导选择、日志同步和持久化三块，都是对照论文和需求文档写的，最主要的是并发线程比较高调试起来也比较困难，花了比较多的时间。

第三个项目是基于Raft实现的单集群分布式数据库，在服务端通过集群的方式保证高可用和内部主从节点的一致性状态。主要是通过每个服务器节点类中有一个Raft对象，然后所有服务器节点的Raft对象按照Raft算法达成共识。对于应用层即数据库方面，这个集群对外需要保证客户端并发请求的强一致性和读请求的幂等性，主要是通过3点实现，第一点是每个客户端的ID唯一；第二点每个客户端的请求递增且服务端收到客户端请求时会把ID->当前请求号存入map，同时生成一个包含上下文信息的指针存入一个ID->上下文的map；做完这些事后把请求传入底层Raft达成共识后通过消息队列回到应用层服务端去执行客户端命令，会先去获取上下文，然后比对请求号和map存的是否相同，再根据具体请求去执行数据库命令。代码量大概在800行，比较关键，因为这样的单个集群功能会延续到最后一个综合项目。

第四个项目是基于Raft实现的多集群分布式分片数据库，主要是在上面高可用强一致性的分布式数据库基础上引入高性能。每个集群就是第三个项目实现的效果，引入分片是为了让客户端请求根据Key散列到不同的集群中，分片类似一致性哈希算法中的节点的概念，一个集群可能管理多个分片，对于某一时刻集群和分片的对应关系，有一个config类来管理，这个类带一个版本号，每当发生集群关系变化，比如加入新集群或者下线集群，就会触发负载均衡，实现分片和集群关系的重映射，这是一个后台线程。其实这个项目又单独分为两个部分，第一个部分需要实现配置管理相关的服务端和客户端功能，包括实现了集群配置的4个API，Join、Leave、Move、Query，以及逻辑上的负载均衡，这一部分其实也是基于Raft实现的高可用集群，同样对客户端请求保证并发强一致性，整体代码在800行，会作为一个模块嵌入到最终项目中的Server类中，通过后台线程不断调用Query去查询配置版本是否变动，若变动则需要发生集群间的数据交换，比如加入一个新的集群，分片经过负载均衡重分配后，新集群需要去拉取原先其他集群维护分片的数据库，而老集群也需要去把自己不再维护分片的数据库清理，节约内存，这就涉及到集群间的数据通信和推拉模式的选择，而且需要保持的是所有集群在同一时刻对于客户端来说所维护的所有分片的数据库是一致的，因为对客户端来说，服务端是个黑盒，只关心自己的操作和数据是否正确。

所以我说我这个项目不太一样，可能我写的这5500行，用spring cloud压根不用写就已经底层实现好了，我相当于这么多代码就是去实现框架的底层功能一样的东西，我做的项目主要是在底层和后端的工作，不太涉及应用层的功能，比如应用层只是简单的set、get、append和有关配置类的4个API罢了，我没有其他意思也不是想和别人比什么的，主要就是想表示我这两个月的工作量和投入容易被忽略，包括很多面试官都是，比较关心我实现了什么功能，但这点我很难去讲清楚。==就比如最后一个项目的并发量和数据交换是比较大的，同时非常多的线程在工作和交换数据，所以保持单集群的一致性状态，多集群集群间的一致性状态，同时还要保证对于客户端请求的强一致性和幂等性，就第四个项目代码就有2800行，实现的粒度很细，不从代码入手很难讲明白。==对于并发强一致性，我的处理比较简单，消息队列的发送需要在应用层处理后才能继续发，通过两个信号量实现，那么不管客户端的命令顺序是什么，谁先到谁后到无所谓，最终到raft的日志系统顺序一旦生成，就会按这个顺序去提交。



### 2、项目中遇到的bug

#### 1、MapReduce

> **1、设置不同的map及reduce线程个数，写入中间文件后reduce线程根据编号读取文件错误**

- 没考虑个位数和十位数的区别，可能有十几个reduce线程那么中间文件命名是两位数
- 根据字符串匹配时简单当作个位数处理
- 花了比较多的时间才想明白，对每个模块都进行了测试才发现到这一步功能与预期不符合



>**2、const char \*和char[]**

- 检测到超时后将runningMapWork中的超时任务重新放回map工作队列m_list中时
- 由于用的是线程中定义的char[]来通过strcpy得到的任务文件名，再push_back到工作链表中
- 导致线程结束后，m_list的back()就成了空，后来用const char \*来存任务名就可以了
- 通过反推法一步一步逆推结合printf发现的



>**3、RPC的阻塞**

- 一开始不熟悉RPC，一个线程中绑定了所有的RPC方法，且在某些方法中用了信号量
- 导致map及reduce对应的worker端发起的RPC请求不能得到很快的响应（任务空了就阻塞）
- 后来就直接简单互斥锁加判断队列是否为空
- 通过printf打印发现程序阻塞的位置发现的bug



#### 2、Raft

>**1、随机时间种子**

- 在每个Raft服务器单独的创建线程中各自随机化时间种子，导致所有server的超时时间一致
- 所以在main函数中全局初始化随机种子即可



>**2、PRC序列化相关**

- 使用的轻量级RPC示例中支持string类型的序列化，但若是string封装到类内需要额外处理
- 同时该框架不支持容器的序列化，所有的容器需要按自己协议转为string在接收端解码重建容器（麻烦）
- 通过gdb调试查看bt信息以及查看段错误的core文件发现的



>**3、timeval还是timespec结构体**

- 由于涉及心跳以及定时选举，故肯定有定时任务
- 发现写完的程序在每个小时的前10分钟和最后十分钟有bug，其他时间正常运行
- 通过printf发现特定时间段的时间输出与预期不符合（忘了怎么改的，错误还原不了了）



>**4、write的系统调用**

- 考虑到字符串结束符，所以采用int len = write(fd, str.c_str(), str.size() + 1)，但会写入一个乱码符
- 直接正常写入str.size()个长度即可
- char\*结合strlen也是如此，加1会有一个奇怪的符号，显示不支持的文本编码



#### 3、KvRaft

>**1、RPC客户端对象**

- 在具体的客户端请求方法中，如Get()，客户端一开始会向服务器列表中轮询谁是leader
- 为了缓存这个leader的ID，以便后续请求直接找这个leader，只要服务端的Raft集群不选举一直有用
- 轮询需要通过RPC客户端对象去发起，若不是则换一个server尝试，若是取得结果退出
- 这个RPC客户端对象若是定义在while外部，导致该函数无法正常退出
- 通过gdb调试解决的问题，虽然依旧不知道为什么是这样的机制



>**2、加入应用层快照后需要对Raft层进行日志压缩**

- 日志压缩导致Raft类的所有涉及下标的代码都需要重新编写，细节太多很多bug
- 新增的快照RPC需要在日志同步的逻辑中额外增加一段，本身进行日志同步的某些服务器可能需要进行快照安装
- 简单的对server数组轮询就不行了，需要区分对待两种处理，用了map加一个标志位解决的bug



#### 4、ShardKV

就是综合调试



### 3、项目中的测试用例

#### 1、MapReduce

- 测试不同的worker个数，即map和reduce线程个数
- 增加map或reduce的宕机节点，看输出结果是否合理
- 宕机后恢复看是否写入重复文件
- 测试边界值，如1个map很多个reduce或多个reduce1个map

#### 2、Raft

- 测试不同奇数个数的Raft集群功能是否正常
- 测试领导选举、日志同步以及持久化各个模块功能是否正常
- 测试让少于半数的节点宕机功能是否正常
- 让宕机节点恢复是否能较快重建状态机
- 多次测试看领导选举是否足够随机

#### 3、KvRaft

- 首先测试单个客户端请求，重复请求
- 测试多客户端并发请求的强一致性，读请求的幂等性
- 测试加入应用层快照后Raft层日志压缩的功能
- 测试Raft层某个节点宕机后的安装快照的功能

#### 4、ShardKv

- 先测试configController部分的功能是否正常，包括客户端的四种请求（Join、Leave、Move、Query）
- 测试单config客户端以及多config客户端并发时的请求是否保证强一致性
- 测试配置更新后负载均衡的逻辑实现是否正常（功能实现是在逻辑实现上进行数据迁移）
- 测试基于分片的多集群分布式数据库系统基本功能是否正常
- 测试对于单数据库客户端请求以及多客户端并发请求的强一致性
- 测试配置更新后负载均衡以及数据迁移和垃圾回收的正确性



### 4、项目改进

说优化也没什么，因为是自己从零实现的不是在谁的基础上改的，唯一就是需求文档里我都是按照较难的处理方式实现性能上的优化吧。做的优化有Raft层日志快速回滚，包括第四个最终项目中的垃圾清理，都是不影响正常功能但在性能上更好的处理方式，实现起来难度比较大吧。当然现在看来还有一些地方需要改进，首先是MapReduce中有一个缺陷，

#### 1、MapReduce

> **针对一个超时map或reduce线程，其可能仅仅是慢了，重发后另一个线程处理该文件并写入中间文件，同时该线程虽然慢但也处理完了**

- 在map工作线程完成mapFunc准备写入中间文件前先check该文件是否已完成
- 具体来说，实现一个新的RPC方法，传入文件名检查单个文件是否已完成（已完成容器中是否存在）
  - 存在的话放弃写入并直接continue
  - 不存在的话将map结果写入中间文件

### 4、ShardKv

>**项目针对的是==高可用性==以及==强一致性==，保证不超过半数节点宕机情况下集群依旧正常工作，对于高性能没太多体现，有体现，分片即体现了**

- 通过分布式常见的主从复制保证高可用，是否可以读写分离，让Follower承担数据库的读压力呢

- 对于客户端请求，只支持数据库方面的get、put、append以及配置方面的join、leave、move、query，是否可以增加事务功能呢

